{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7631086cdf7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import urllib\n",
    "import csv\n",
    "from urllib import urlopen\n",
    "\n",
    "# from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "contents = []\n",
    "\n",
    "f = csv.writer(open('20200707.csv', 'w'))\n",
    "# f.writerow(['Title','Date','Publisher','Description','Download']) to be deleted\n",
    "\n",
    "with open('plssLinks.csv','r') as csvf: # Open file in read mode\n",
    "    urls = csv.reader(csvf)\n",
    "    for url in urls:\n",
    "        contents.append(url) # Add each url to list contents\n",
    "\n",
    "for url in contents:  # Parse through each url in the list.\n",
    "    page = urlopen(url[0]).read()\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    table = soup.find('table', attrs={'class':'lineItemsTable'})\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        contents.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "# \ttitleField = soup.find(attrs={'id': 'Label1'})\n",
    "# \tdateField = soup.find(attrs={'id': 'Label2'})\n",
    "# \tpublisherField = soup.find(attrs={'id': 'Label3'})\n",
    "# \tdescriptionField = soup.find(attrs={'id': 'Label14'})\n",
    "# # \tmetadataLink = soup.find('a', href=True, text='Metadata')\n",
    "#  \tdownloadLink = soup.find('a', href=True, text='Download historic versions of this dataset')\n",
    "\n",
    "# \ttitle = titleField.text.strip(),\n",
    "# \tdate = dateField.text.strip(),\n",
    "# \tpublisher = publisherField.text.strip(),\n",
    "# \tdescription = descriptionField.text.strip(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
